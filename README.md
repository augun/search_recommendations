# 🔍 MS MARCO Two-Tower Search Engine

This project builds a neural search engine using the MS MARCO v1.1 dataset and a Two-Tower architecture trained with Triplet Loss.

---

## 🚀 Project Overview

We train a model to learn vector representations of queries and documents, so that similar queries and relevant documents are close in vector space. This allows us to use cosine similarity to rank documents for any given query.

The core steps are:

1. **Prepare dataset**: Extract query-document triplets
2. **Train Two-Tower model**: With RNN-based encoders for query and document towers
3. **Pre-cache document encodings**: Speed up retrieval
4. **Run fast search**: Use cosine similarity to find the best matches

---

## 🛠️ Setup

### 1. Install dependencies
```bash
pip install torch transformers datasets tqdm
```

---

## 📁 Project Structure

```bash
search_recommendations/
├── dataset_preparation.py      # Prepares triplet dataset from MS MARCO
├── two_tower.py                # Defines the Two-Tower model
├── train.py                    # Trains the model using Triplet Loss
├── cache_doc_embeddings.py     # Encodes & saves document vectors for fast search
├── search_in_cached.py         # Runs inference and ranks documents by similarity
├── two_tower_model.pt          # Saved trained model (generated by train.py)
├── doc_vectors.pt              # Cached document embeddings (generated by cache_doc_embeddings.py)
├── doc_texts.pkl               # Corresponding passage texts
```

---

## ⚙️ Step-by-Step Instructions

### 1. Prepare triplet dataset
This is handled inside `dataset_preparation.py` using MS MARCO's `passage_text` and `is_selected` fields.

```bash
python dataset_preparation.py
```

### 2. Train the Two-Tower model
This will create `two_tower_model.pt`.
```bash
python train.py
```

### 3. Cache document embeddings
Encode passages only once for fast future searches.
```bash
python cache_doc_embeddings.py
```

### 4. Run search over cached data
Find top-matching documents using cosine similarity.
```bash
python search_in_cached.py
```

---

## 📊 Model Architecture

- Word embeddings (BERT tokenizer)
- RNN (GRU) encoder for queries
- RNN (GRU) encoder for documents
- Triplet Margin Loss for training
- Cosine similarity for retrieval

---

## ✅ Example Query

```
Query: what is results-based accountability
```

Results:
```
1. [Score: 0.8942] Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking...
2. [Score: 0.8250] RBA is also used by organizations to improve the performance of their programs...
...
```

---

## 👤 Git Setup Reminder
```bash
git config --global user.name "Aygun"
git config --global user.email "aygun1987@gmail.com"
```

---

## 📌 Next Ideas
- Swap GRU with BERT encoder
- Add FAISS for large-scale vector search
- Build an API or web UI with Flask or FastAPI

---

## 🙌 Credits
Project inspired by deep learning search architectures & the MS MARCO dataset.

